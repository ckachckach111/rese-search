import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „ ì²˜ë¦¬)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì½”ë”©ì„ UTF-8(CSV UTF-8)ë¡œ ì €ì¥í•´ ë³´ì„¸ìš”.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •(ì•½ê°„ì˜ ìœ ì—°ì„±)
# -------------------------
# ê¸°ëŒ€ ì»¬ëŸ¼ ì´ë¦„
expected_cols = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]

# ê°„ë‹¨íˆ ê³µë°±/ëª…ì¹­ ì°¨ì´ ë³´ì •: 'ìºë¦­í„°ëª©ë¡' -> 'ìºë¦­í„° ëª©ë¡'
cols = list(df.columns)
cols_fixed = []
for c in cols:
    cc = c.strip()
    if cc == "ìºë¦­í„°ëª©ë¡":
        cc = "ìºë¦­í„° ëª©ë¡"
    cols_fixed.append(cc)
df.columns = cols_fixed

missing = [c for c in expected_cols if c not in df.columns]
if missing:
    st.error(f"CSVì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: {missing}  â€” íŒŒì¼ì˜ í—¤ë”(ë§¨ ìœ—ì¤„)ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -------------------------
# í…ìŠ¤íŠ¸ ì •ê·œí™” ë° í† í°í™” í•¨ìˆ˜ (ê°•í™”)
# -------------------------
def normalize_text_for_tokens(s: str):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ê³  í† í° ë¦¬ìŠ¤íŠ¸(ì†Œë¬¸ì ì˜ë¬¸/ìˆ«ì, í•œê¸€ í† í°) ë°˜í™˜"""
    if not isinstance(s, str):
        return []
    # 1. Unicode NFKC (ì „ê°->ë°˜ê° ë“±)
    s = unicodedata.normalize("NFKC", s)
    # 2. ê³µë°± í†µì¼
    s = s.replace("\u3000", " ").replace("\xa0", " ").strip()
    # 3. ì œê±°í•  íŒ¨í„´ (ì•ˆì „í•˜ê²Œ)
    s = re.sub(r'\(\+\s*\d+\s*\)', ' ', s)
    s = re.sub(r'\+\s*\d+', ' ', s)
    s = re.sub(r'\b[45]\s*ì„±\b', ' ', s)  # 4ì„±, 5ì„± ì œê±°
    # 4. ê´„í˜¸/êµ¬ë¶„ì/íŠ¹ìˆ˜ë¬¸ì -> ê³µë°±
    s = re.sub(r'[,\|/Â·;:()\[\]ã€ã€‘\-\â€”\_\/\\\*â€¢â€¦Â·"\'`~<>Â«Â»]', ' ', s)
    # 5. ì´ëª¨ì§€ ì œê±° (ëŒ€ëµ)
    s = re.sub(r'[\U00010000-\U0010ffff]', ' ', s)
    # 6. ì—°ì† ê³µë°± ì •ë¦¬
    s = re.sub(r'\s+', ' ', s).strip()
    # 7. í† í°í™”: í•œê¸€ ì—°ì† ë¬¸ìì—´ OR ì˜ë¬¸/ìˆ«ì ì—°ì†
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', s)
    tokens = [t.lower() for t in tokens if t.strip()]
    return tokens

def normalize_search_terms(raw_query: str):
    """ì‚¬ìš©ì ì…ë ¥ì„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ê²€ìƒ‰ í† í° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if not raw_query or not str(raw_query).strip():
        return []
    parts = [p for p in re.split(r'[\s,]+', raw_query.strip()) if p]
    normalized = []
    for p in parts:
        p2 = unicodedata.normalize("NFKC", p)
        p2 = p2.replace("\u3000", " ").replace("\xa0", " ").strip()
        p2 = re.sub(r'\b[45]\s*ì„±\b', '', p2)
        p2 = re.sub(r'\+\s*\d+', '', p2)
        # ì˜ë¬¸/ìˆ«ì/í•œê¸€ë§Œ ë‚¨ê¹€
        p2 = re.sub(r'[^ê°€-í£A-Za-z0-9]', '', p2)
        p2 = p2.lower().strip()
        if p2:
            # further tokenization if composite, but keep simple:
            # e.g., 'ìš°ë¯¸ì¹´' -> 'ìš°ë¯¸ì¹´'
            normalized.append(p2)
    return normalized

# -------------------------
# í† í° ì»¬ëŸ¼ ìƒì„± (ìºì‹œëœ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±)
# -------------------------
if "_tokens" not in df.columns:
    df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").astype(str).apply(normalize_text_for_tokens)

# -------------------------
# UI ì„¤ì •
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰", layout="centered")
st.markdown("""
    <style>
        .stTextInput { text-align: center; }
        div[data-testid="stTextInput"] label { display: none; }
        .block-container { display:flex; flex-direction:column; justify-content:center; align-items:center; }
        div[data-testid="stDataFrame"] td { white-space: pre-wrap !important; word-break: break-word !important; }
        div[data-testid="stDataFrame"] td:nth-child(4) { min-width: 450px !important; }
    </style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰</h2>", unsafe_allow_html=True)

# session state for search
if "search_clicked" not in st.session_state:
    st.session_state.search_clicked = False

# ì…ë ¥ì°½ ë° í•„í„°
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬ (ë„ì–´ì“°ê¸°ë¡œ ì—¬ëŸ¬ ìºë¦­í„° ê²€ìƒ‰)")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100), step=1)
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ê°œìˆ˜", min_value=0, max_value=100, value=0, step=1)

if st.button("ê²€ìƒ‰"):
    st.session_state.search_clicked = True

# -------------------------
# ê²€ìƒ‰ ì²˜ë¦¬
# -------------------------
if st.session_state.search_clicked:
    # normalize search tokens
    search_tokens = normalize_search_terms(query)
    filtered = df.copy()

    # ê°€ê²© í•„í„° (numeric ì•ˆì „ ì²˜ë¦¬)
    filtered["ê°€ê²©"] = pd.to_numeric(filtered["ê°€ê²©"], errors="coerce")
    filtered = filtered[(filtered["ê°€ê²©"] >= min_price) & (filtered["ê°€ê²©"] <= max_price)]

    # í•œì • í•„í„°
    filtered["í•œì •"] = pd.to_numeric(filtered["í•œì •"], errors="coerce").fillna(0)
    filtered = filtered[filtered["í•œì •"] >= min_limit]

    # í† í° ê¸°ë°˜ AND ë§¤ì¹­: ëª¨ë“  ê²€ìƒ‰ í† í°ì´ _tokensì— í¬í•¨ë˜ì–´ì•¼ í•¨
    if search_tokens:
        def match_all_tokens(token_list):
            if not isinstance(token_list, (list, tuple)):
                return False
            # ì •í™• ì¼ì¹˜ í† í°(ì†Œë¬¸ì)
            return all(any(term == tok for tok in token_list) for term in search_tokens)
        filtered = filtered[filtered["_tokens"].apply(match_all_tokens)]

    # ê²°ê³¼ ì¶œë ¥
    if not filtered.empty:
        st.write(f"ğŸ” ì´ {len(filtered)}ê°œ ê³„ì •ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.dataframe(filtered[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]], use_container_width=True, height=600)
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë””ë²„ê·¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ë””ë²„ê·¸: ìƒ˜í”Œ + ê·¼ì ‘ ë§¤ì¹­ í›„ë³´
        st.markdown("**ë””ë²„ê·¸: CSV ìƒ˜í”Œ (í† í° í¬í•¨)**")
        sample = df.head(10)[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡", "_tokens"]]
        st.dataframe(sample, use_container_width=True, height=300)

        if search_tokens:
            st.markdown("**ë””ë²„ê·¸: ê²€ìƒ‰ í† í°**")
            st.write(search_tokens)

            st.markdown("**ë””ë²„ê·¸: ê·¼ì ‘ í›„ë³´ (í† í° í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ëŠ” í–‰, ìµœëŒ€ 30í–‰)**")
            def any_match(token_list):
                if not isinstance(token_list, (list, tuple)):
                    return False
                return any(term in token_list for term in search_tokens)
            near = df[df["_tokens"].apply(any_match)].head(30)[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡", "_tokens"]]
            if not near.empty:
                st.dataframe(near, use_container_width=True, height=400)
            else:
                st.info("ê·¼ì ‘ í›„ë³´ ì—†ìŒ. CSVì˜ ë¬¸ì(íŠ¹ìˆ˜ë¬¸ì/ë¶™ì–´ìˆëŠ” í˜•íƒœ ë“±)ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ì‚¬ìš© ë°©ë²•
st.markdown("""
---
### ğŸ’¡ ì‚¬ìš© ë°©ë²•
1ï¸âƒ£ ê²€ìƒ‰ì°½ì— ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•´ ì…ë ¥í•˜ì„¸ìš”.  
â€ƒâ€ƒì˜ˆ: `íˆë§ˆë¦¬ íˆì¹´ë¦¬ ë„¤ë£¨`  
2ï¸âƒ£ ê°€ê²©ëŒ€ ìŠ¬ë¼ì´ë”ë¡œ ì›í•˜ëŠ” ê°€ê²© ë²”ìœ„ë¥¼ ì„¤ì •í•˜ì„¸ìš”.  
3ï¸âƒ£ â€˜ìµœì†Œ í•œì • ìºë¦­í„° ê°œìˆ˜â€™ë¥¼ ì¡°ì •í•˜ì—¬ ì¡°ê±´ì„ ì„¸ë°€í•˜ê²Œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
4ï¸âƒ£ [ê²€ìƒ‰] ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ í‘œë¡œ í‘œì‹œë©ë‹ˆë‹¤.  
5ï¸âƒ£ ìºë¦­í„° ëª©ë¡ì€ í‘œì—ì„œ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
""", unsafe_allow_html=True)
