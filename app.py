import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ê³¼ ì¸ì½”ë”©ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •
# -------------------------
df.columns = [c.strip().replace("ìºë¦­í„°ëª©ë¡", "ìºë¦­í„° ëª©ë¡") for c in df.columns]
required = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]
if any(c not in df.columns for c in required):
    st.error(f"CSV ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {required} í˜„ì¬: {list(df.columns)}")
    st.stop()

# -------------------------
# ì •ê·œí™” & í† í°í™” í•¨ìˆ˜
# -------------------------
def normalize_text_for_tokens(s: str):
    if not isinstance(s, str):
        return []
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\u3000", " ").replace("\xa0", " ").strip()
    # ì œê±°: (+ìˆ«ì), +ìˆ«ì, 4ì„±/5ì„±
    s = re.sub(r'\(\+\s*\d+\s*\)', ' ', s)
    s = re.sub(r'\+\s*\d+', ' ', s)
    s = re.sub(r'\b[45]\s*ì„±\b', ' ', s)
    # íŠ¹ìˆ˜ë¬¸ì -> ê³µë°±
    s = re.sub(r'[,\|/Â·;:()\[\]ã€ã€‘\-\â€”\_\/\\\*â€¢â€¦Â·"\'`~<>Â«Â»]', ' ', s)
    # ì´ëª¨ì§€ ë“± ì œê±°(ëŒ€ëµ)
    s = re.sub(r'[\U00010000-\U0010ffff]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    # í† í°í™”: í•œê¸€ ì—°ì† ë˜ëŠ” ì˜ë¬¸/ìˆ«ì ì—°ì†
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', s)
    return [t.lower() for t in tokens if t.strip()]

def normalize_search_terms(raw_query: str):
    if not raw_query or not str(raw_query).strip():
        return []
    parts = [p for p in re.split(r'[\s,]+', raw_query.strip()) if p]
    normalized = []
    for p in parts:
        p2 = unicodedata.normalize("NFKC", p)
        p2 = p2.replace("\u3000", " ").replace("\xa0", " ").strip()
        p2 = re.sub(r'\b[45]\s*ì„±\b', '', p2)
        p2 = re.sub(r'\+\s*\d+', '', p2)
        p2 = re.sub(r'[^ê°€-í£A-Za-z0-9]', '', p2)
        p2 = p2.lower().strip()
        if p2:
            normalized.append(p2)
    return normalized

# -------------------------
# í˜ìŠ¤ ìºë¦­í„° ë¦¬ìŠ¤íŠ¸ (ìš”ì²­í•œ ëª©ë¡ì— ìˆ˜ê¸°ì‚¬, ìˆ˜ë¯¸ì¹´ ì¶”ê°€)
# -------------------------
fes_list = [
    "ë¦¬ì˜¤", "êµë„¤ë£¨", "ì„ì‹œë…¸", "ì¿ ë¡œì½”", "ë“œíˆë‚˜",
    "ìˆ˜ë‚˜ì½”", "ë¯¸ì¹´", "ìˆ˜ì‹œë…¸", "ì™€ì¹´ëª¨", "ìˆ˜ê¸°ì‚¬", "ìˆ˜ë¯¸ì¹´"
]
# normalize fes names same way as tokens (lower-case, NFKC)
fes_names_norm = [unicodedata.normalize("NFKC", n).lower() for n in fes_list]

# -------------------------
# í† í° ì»¬ëŸ¼ ìƒì„± (ì „ì²´ í…ìŠ¤íŠ¸) â€” ê²€ìƒ‰ìš©
# -------------------------
df["ìºë¦­í„° ëª©ë¡"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").astype(str)
df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].apply(normalize_text_for_tokens)

# -------------------------
# íŒ¨ìŠ¤ ê°¯ìˆ˜: CSVì˜ ê°’ ì‚¬ìš© (ê¸°ì¡´ ì˜ëª»ëœ ê³„ì‚° ì‚­ì œ)
# -------------------------
# ìš°ì„  í›„ë³´ ì»¬ëŸ¼ëª… ì¤‘ í•˜ë‚˜ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
pass_col = None
for candidate in ["íŒ¨ìŠ¤ ê°¯ìˆ˜", "íŒ¨ìŠ¤", "íŒ¨ìŠ¤ê°¯ìˆ˜"]:
    if candidate in df.columns:
        pass_col = candidate
        break

if pass_col is None:
    # íŒ¨ìŠ¤ ê°¯ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê²½ê³  í›„ ê¸°ë³¸ 0ìœ¼ë¡œ ì„¤ì •
    st.warning("CSVì— 'íŒ¨ìŠ¤ ê°¯ìˆ˜' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒ¨ìŠ¤ í•„í„°ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    df["íŒ¨ìŠ¤"] = 0
else:
    # CSVì— ì íŒ ê°’ì„ ê·¸ëŒ€ë¡œ ìˆ«ìë¡œ ì½ì–´ì„œ df['íŒ¨ìŠ¤']ì— ë„£ìŒ
    df["íŒ¨ìŠ¤"] = pd.to_numeric(df[pass_col], errors="coerce").fillna(0).astype(int)

# -------------------------
# í˜ìŠ¤ìºë¦­í„°ìˆ˜: CSVì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í•œì • ë¸”ë¡ì—ì„œ ê³„ì‚°
# -------------------------
if "í˜ìŠ¤ìºë¦­í„°ìˆ˜" in df.columns:
    df["í˜ìŠ¤ìºë¦­í„°ìˆ˜"] = pd.to_numeric(df["í˜ìŠ¤ìºë¦­í„°ìˆ˜"], errors="coerce").fillna(0).astype(int)
else:
    # extract limited block inside ã€...ã€‘ and count fes names
    def extract_limited_block(s: str):
        m = re.search(r'ã€([^ã€‘]*)ã€‘', s)
        if m:
            return m.group(1)
        if '/' in s:
            return s.split('/', 1)[0]
        return s
    df["_limited_text"] = df["ìºë¦­í„° ëª©ë¡"].apply(extract_limited_block)
    df["_limited_tokens"] = df["_limited_text"].apply(normalize_text_for_tokens)
    def count_fes_chars_in_limited(tokens):
        if not isinstance(tokens, (list, tuple)):
            return 0
        cnt = 0
        for name in fes_names_norm:
            if name in tokens:
                cnt += 1
        return cnt
    df["í˜ìŠ¤ìºë¦­í„°ìˆ˜"] = df["_limited_tokens"].apply(count_fes_chars_in_limited)

# -------------------------
# UI ìŠ¤íƒ€ì¼ (í…Œì´ë¸” ë„“ì´ ë“±)
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰ (íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)", layout="wide")
st.markdown("""
<style>
div[data-testid="stDataFrame"] table { width: 100% !important; }
div[data-testid="stDataFrame"] td { white-space: pre-wrap !important; word-break: break-word !important; line-height:1.35em !important; }
div[data-testid="stDataFrame"] td:nth-child(4) { min-width: 600px !important; max-width: 1000px !important; }
</style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰ (íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)</h2>", unsafe_allow_html=True)

# -------------------------
# ê²€ìƒ‰ ì¡°ê±´ UI
# -------------------------
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬ (ë„ì–´ì“°ê¸°ë¡œ AND ê²€ìƒ‰)")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100))
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ìˆ˜", 0, 100, 0)

# íŒ¨ìŠ¤ ë²”ìœ„ ìŠ¬ë¼ì´ë”: CSVì˜ 'íŒ¨ìŠ¤' ê°’(ì´ë¯¸ df['íŒ¨ìŠ¤']ë¡œ ë§¤í•‘ë¨)ì„ ì‚¬ìš©
min_pass_possible = int(df["íŒ¨ìŠ¤"].min()) if not df["íŒ¨ìŠ¤"].isnull().all() else 0
max_pass_possible = int(df["íŒ¨ìŠ¤"].max()) if not df["íŒ¨ìŠ¤"].isnull().all() else 0
min_pass, max_pass = st.slider("íŒ¨ìŠ¤ ê°¯ìˆ˜ (CSVê°’)", min_pass_possible, max_pass_possible, (min_pass_possible, max_pass_possible))

min_fes = st.number_input("ìµœì†Œ í˜ìŠ¤ ìºë¦­í„° ìˆ˜", min_value=0, max_value=20, value=0, step=1)

if st.button("ê²€ìƒ‰"):
    result = df.copy()
    # ê°€ê²©/í•œì • í•„í„° (ìˆ«ì ì•ˆì „)
    result["ê°€ê²©"] = pd.to_numeric(result["ê°€ê²©"], errors="coerce")
    result = result[(result["ê°€ê²©"] >= min_price) & (result["ê°€ê²©"] <= max_price)]
    result["í•œì •"] = pd.to_numeric(result["í•œì •"], errors="coerce").fillna(0)
    result = result[result["í•œì •"] >= min_limit]

    # íŒ¨ìŠ¤ í•„í„°: CSVì˜ 'íŒ¨ìŠ¤' ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    result = result[(result["íŒ¨ìŠ¤"] >= min_pass) & (result["íŒ¨ìŠ¤"] <= max_pass)]

    # í˜ìŠ¤ í•„í„°
    result = result[result["í˜ìŠ¤ìºë¦­í„°ìˆ˜"] >= min_fes]

    # ìºë¦­í„° AND ê²€ìƒ‰ (í† í° ë‹¨ìœ„ ì •í™•ë§¤ì¹­) â€” ì „ì²´ ëª©ë¡ ê¸°ì¤€
    terms = normalize_search_terms(query)
    if terms:
        result = result[result["_tokens"].apply(lambda toks: all(term in toks for term in terms))]

    # ê²°ê³¼ ì¶œë ¥
    if not result.empty:
        st.write(f"ğŸ” ì´ {len(result)}ê°œ ê³„ì • (í‘œì— íŒ¨ìŠ¤ / í˜ìŠ¤ìºë¦­í„°ìˆ˜ í¬í•¨)")
        st.dataframe(result[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "í˜ìŠ¤ìºë¦­í„°ìˆ˜", "ìºë¦­í„° ëª©ë¡"]], use_container_width=True, height=700)
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("**ë””ë²„ê·¸: CSV ìƒìœ„ 10í–‰ (í† í°/íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)**")
        debug_cols = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "í˜ìŠ¤ìºë¦­í„°ìˆ˜", "ìºë¦­í„° ëª©ë¡", "_limited_text", "_limited_tokens", "_tokens"]
        st.dataframe(df.head(10)[[c for c in debug_cols if c in df.columns]], use_container_width=True, height=400)

# ì‚¬ìš© ë°©ë²•
st.markdown("""
---
### ğŸ’¡ ì‚¬ìš© ë°©ë²•
1ï¸âƒ£ ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ ì…ë ¥í•˜ë©´ **AND ê²€ìƒ‰**ë©ë‹ˆë‹¤. (í† í° ë‹¨ìœ„ ì •í™• ë§¤ì¹­)  
2ï¸âƒ£ ê°€ê²© / í•œì • / íŒ¨ìŠ¤(CSVê°’) / í˜ìŠ¤ìºë¦­í„°ìˆ˜ ë¡œ í•„í„°ë§ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
3ï¸âƒ£ ê²°ê³¼ í‘œì—ì„œ `íŒ¨ìŠ¤`(CSVê°’)ì™€ `í˜ìŠ¤ìºë¦­í„°ìˆ˜`ë¥¼ í™•ì¸í•˜ì„¸ìš”.
""", unsafe_allow_html=True)
