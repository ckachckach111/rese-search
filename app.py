import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •
# -------------------------
df.columns = [c.strip().replace("ìºë¦­í„°ëª©ë¡", "ìºë¦­í„° ëª©ë¡") for c in df.columns]
required = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]
if any(c not in df.columns for c in required):
    st.error("CSV ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# -------------------------
# ì •ê·œí™” & í† í°í™”
# -------------------------
def normalize_tokens(text):
    if not isinstance(text, str):
        return []
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r'[^\wê°€-í£\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', text.lower())

df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").apply(normalize_tokens)

# âœ… íŒ¨ìŠ¤ ê°¯ìˆ˜ ì»¬ëŸ¼ ìƒì„± (ìºë¦­í„° ìˆ˜)
df["íŒ¨ìŠ¤"] = df["_tokens"].apply(len)

# -------------------------
# UI
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰", layout="wide")

st.markdown("""
<style>
div[data-testid="stDataFrame"] td {
    white-space: pre-wrap !important;
    word-break: break-word !important;
}
div[data-testid="stDataFrame"] td:nth-child(4) {
    min-width: 650px !important;
}
</style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰</h2>", unsafe_allow_html=True)

# -------------------------
# ê²€ìƒ‰ ì¡°ê±´
# -------------------------
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100))
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ìˆ˜", 0, 50, 0)

min_pass, max_pass = st.slider(
    "íŒ¨ìŠ¤ ê°¯ìˆ˜ (ìºë¦­í„° ìˆ˜)",
    int(df["íŒ¨ìŠ¤"].min()),
    int(df["íŒ¨ìŠ¤"].max()),
    (int(df["íŒ¨ìŠ¤"].min()), int(df["íŒ¨ìŠ¤"].max()))
)

if st.button("ê²€ìƒ‰"):
    result = df.copy()

    # ê°€ê²©
    result["ê°€ê²©"] = pd.to_numeric(result["ê°€ê²©"], errors="coerce")
    result = result[(result["ê°€ê²©"] >= min_price) & (result["ê°€ê²©"] <= max_price)]

    # í•œì •
    result["í•œì •"] = pd.to_numeric(result["í•œì •"], errors="coerce").fillna(0)
    result = result[result["í•œì •"] >= min_limit]

    # íŒ¨ìŠ¤
    result = result[(result["íŒ¨ìŠ¤"] >= min_pass) & (result["íŒ¨ìŠ¤"] <= max_pass)]

    # ìºë¦­í„° AND ê²€ìƒ‰
    terms = normalize_tokens(query)
    if terms:
        result = result[result["_tokens"].apply(
            lambda toks: all(t in toks for t in terms)
        )]

    # -------------------------
    # ê²°ê³¼
    # -------------------------
    if not result.empty:
        st.write(f"ğŸ” ì´ {len(result)}ê°œ ê³„ì •")
        st.dataframe(
            result[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "ìºë¦­í„° ëª©ë¡"]],
            use_container_width=True,
            height=700
        )
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

# -------------------------
# ì‚¬ìš© ë°©ë²•
# -------------------------
st.markdown("""
---
### ğŸ’¡ ì‚¬ìš© ë°©ë²•
1ï¸âƒ£ ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ ì…ë ¥í•˜ë©´ **AND ê²€ìƒ‰**ë©ë‹ˆë‹¤  
2ï¸âƒ£ ê°€ê²© / í•œì • / **íŒ¨ìŠ¤ ê°¯ìˆ˜(ìºë¦­í„° ìˆ˜)** ë¡œ í•„í„°ë§ ê°€ëŠ¥  
3ï¸âƒ£ ê²°ê³¼ í…Œì´ë¸”ì—ì„œ ìºë¦­í„° ëª©ë¡ì„ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
""")
