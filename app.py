import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ê³¼ ì¸ì½”ë”©ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •
# -------------------------
df.columns = [c.strip().replace("ìºë¦­í„°ëª©ë¡", "ìºë¦­í„° ëª©ë¡") for c in df.columns]
required = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]
if any(c not in df.columns for c in required):
    st.error(f"CSV ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {required} í˜„ì¬: {list(df.columns)}")
    st.stop()

# -------------------------
# ì •ê·œí™” & í† í°í™” í•¨ìˆ˜
# -------------------------
def normalize_text_for_tokens(s: str):
    if not isinstance(s, str):
        return []
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\u3000", " ").replace("\xa0", " ").strip()
    # ì œê±°: (+ìˆ«ì), +ìˆ«ì, 4ì„±/5ì„±
    s = re.sub(r'\(\+\s*\d+\s*\)', ' ', s)
    s = re.sub(r'\+\s*\d+', ' ', s)
    s = re.sub(r'\b[45]\s*ì„±\b', ' ', s)
    # íŠ¹ìˆ˜ë¬¸ì -> ê³µë°±
    s = re.sub(r'[,\|/Â·;:()\[\]ã€ã€‘\-\â€”\_\/\\\*â€¢â€¦Â·"\'`~<>Â«Â»]', ' ', s)
    # ì´ëª¨ì§€ ë“± ì œê±°(ëŒ€ëµ)
    s = re.sub(r'[\U00010000-\U0010ffff]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    # í† í°í™”: í•œê¸€ ì—°ì† ë˜ëŠ” ì˜ë¬¸/ìˆ«ì ì—°ì†
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', s)
    return [t.lower() for t in tokens if t.strip()]

def normalize_search_terms(raw_query: str):
    if not raw_query or not str(raw_query).strip():
        return []
    parts = [p for p in re.split(r'[\s,]+', raw_query.strip()) if p]
    normalized = []
    for p in parts:
        p2 = unicodedata.normalize("NFKC", p)
        p2 = p2.replace("\u3000", " ").replace("\xa0", " ").strip()
        p2 = re.sub(r'\b[45]\s*ì„±\b', '', p2)
        p2 = re.sub(r'\+\s*\d+', '', p2)
        p2 = re.sub(r'[^ê°€-í£A-Za-z0-9]', '', p2)
        p2 = p2.lower().strip()
        if p2:
            normalized.append(p2)
    return normalized

# -------------------------
# í˜ìŠ¤ ìºë¦­í„° ë¦¬ìŠ¤íŠ¸ (ìš”ì²­í•œ ëª©ë¡ì— ìˆ˜ê¸°ì‚¬, ìˆ˜ë¯¸ì¹´ ì¶”ê°€)
# -------------------------
fes_list = [
    "ë¦¬ì˜¤", "êµë„¤ë£¨", "ì„ì‹œë…¸", "ì¿ ë¡œì½”", "ë“œíˆë‚˜",
    "ìˆ˜ë‚˜ì½”", "ë¯¸ì¹´", "ìˆ˜ì‹œë…¸", "ì™€ì¹´ëª¨", "ìˆ˜ê¸°ì‚¬", "ìˆ˜ë¯¸ì¹´"
]
# normalize fes names same way as tokens (lower-case, NFKC)
fes_names_norm = [unicodedata.normalize("NFKC", n).lower() for n in fes_list]

# -------------------------
# í† í° ì»¬ëŸ¼, íŒ¨ìŠ¤, í˜ìŠ¤ìºë¦­í„°ìˆ˜ ìƒì„± (ìˆìœ¼ë©´ ë®ì–´ì”Œì›€)
# -------------------------
df["ìºë¦­í„° ëª©ë¡"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").astype(str)
df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].apply(normalize_text_for_tokens)
df["íŒ¨ìŠ¤"] = df["_tokens"].apply(len)

def count_fes_chars(tokens):
    if not isinstance(tokens, (list, tuple)):
        return 0
    # tokens are lower-case
    return sum(1 for name in fes_names_norm if name in tokens)

df["í˜ìŠ¤ìºë¦­í„°ìˆ˜"] = df["_tokens"].apply(count_fes_chars)

# -------------------------
# UI ìŠ¤íƒ€ì¼ (í…Œì´ë¸” ë„“ì´ ë“±)
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰ (íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)", layout="wide")
st.markdown("""
<style>
div[data-testid="stDataFrame"] table { width: 100% !important; }
div[data-testid="stDataFrame"] td { white-space: pre-wrap !important; word-break: break-word !important; line-height:1.35em !important; }
div[data-testid="stDataFrame"] td:nth-child(4) { min-width: 600px !important; max-width: 1000px !important; }
</style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰ (íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)</h2>", unsafe_allow_html=True)

# -------------------------
# ê²€ìƒ‰ ì¡°ê±´ UI
# -------------------------
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬ (ë„ì–´ì“°ê¸°ë¡œ AND ê²€ìƒ‰)")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100))
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ìˆ˜", 0, 100, 0)
# íŒ¨ìŠ¤ ë²”ìœ„ ìŠ¬ë¼ì´ë”: df ê°’ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸°í™”
min_pass_possible = int(df["íŒ¨ìŠ¤"].min()) if not df["íŒ¨ìŠ¤"].isnull().all() else 0
max_pass_possible = int(df["íŒ¨ìŠ¤"].max()) if not df["íŒ¨ìŠ¤"].isnull().all() else 0
min_pass, max_pass = st.slider("íŒ¨ìŠ¤ ê°¯ìˆ˜ (ìºë¦­í„° ìˆ˜)", min_pass_possible, max_pass_possible, (min_pass_possible, max_pass_possible))
min_fes = st.number_input("ìµœì†Œ í˜ìŠ¤ ìºë¦­í„° ìˆ˜", min_value=0, max_value=20, value=0, step=1)

if st.button("ê²€ìƒ‰"):
    result = df.copy()
    # ê°€ê²©/í•œì • í•„í„° (ìˆ«ì ì•ˆì „)
    result["ê°€ê²©"] = pd.to_numeric(result["ê°€ê²©"], errors="coerce")
    result = result[(result["ê°€ê²©"] >= min_price) & (result["ê°€ê²©"] <= max_price)]
    result["í•œì •"] = pd.to_numeric(result["í•œì •"], errors="coerce").fillna(0)
    result = result[result["í•œì •"] >= min_limit]
    # íŒ¨ìŠ¤ í•„í„°
    result = result[(result["íŒ¨ìŠ¤"] >= min_pass) & (result["íŒ¨ìŠ¤"] <= max_pass)]
    # í˜ìŠ¤ í•„í„°
    result = result[result["í˜ìŠ¤ìºë¦­í„°ìˆ˜"] >= min_fes]
    # ìºë¦­í„° AND ê²€ìƒ‰ (í† í° ë‹¨ìœ„ ì •í™•ë§¤ì¹­)
    terms = normalize_search_terms(query)
    if terms:
        result = result[result["_tokens"].apply(lambda toks: all(term in toks for term in terms))]
    # ê²°ê³¼ ì¶œë ¥
    if not result.empty:
        st.write(f"ğŸ” ì´ {len(result)}ê°œ ê³„ì • (í‘œì— íŒ¨ìŠ¤ / í˜ìŠ¤ìºë¦­í„°ìˆ˜ í¬í•¨)")
        st.dataframe(result[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "í˜ìŠ¤ìºë¦­í„°ìˆ˜", "ìºë¦­í„° ëª©ë¡"]], use_container_width=True, height=700)
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("**ë””ë²„ê·¸: CSV ìƒìœ„ 10í–‰ (í† í°/íŒ¨ìŠ¤/í˜ìŠ¤ í¬í•¨)**")
        st.dataframe(df.head(10)[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "í˜ìŠ¤ìºë¦­í„°ìˆ˜", "ìºë¦­í„° ëª©ë¡", "_tokens"]], use_container_width=True, height=400)

# ì‚¬ìš© ë°©ë²•
st.markdown("""
---
### ğŸ’¡ ì‚¬ìš© ë°©ë²•
1ï¸âƒ£ ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ ì…ë ¥í•˜ë©´ **AND ê²€ìƒ‰**ë©ë‹ˆë‹¤. (í† í° ë‹¨ìœ„ ì •í™• ë§¤ì¹­)  
2ï¸âƒ£ ê°€ê²© / í•œì • / íŒ¨ìŠ¤(ìºë¦­í„° ìˆ˜) / í˜ìŠ¤ìºë¦­í„°ìˆ˜ ë¡œ í•„í„°ë§ ê°€ëŠ¥í•©ë‹ˆë‹¤.  
3ï¸âƒ£ ê²°ê³¼ í‘œì—ì„œ `íŒ¨ìŠ¤`(ì´ ìºë¦­í„° ìˆ˜)ì™€ `í˜ìŠ¤ìºë¦­í„°ìˆ˜`ë¥¼ í™•ì¸í•˜ì„¸ìš”.
""", unsafe_allow_html=True)
