import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „ ì²˜ë¦¬)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì½”ë”©ì„ UTF-8(CSV UTF-8)ë¡œ ì €ì¥í•´ ë³´ì„¸ìš”.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •
# -------------------------
expected_cols = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]
cols_fixed = []
for c in df.columns:
    cc = c.strip()
    if cc == "ìºë¦­í„°ëª©ë¡":
        cc = "ìºë¦­í„° ëª©ë¡"
    cols_fixed.append(cc)
df.columns = cols_fixed

missing = [c for c in expected_cols if c not in df.columns]
if missing:
    st.error(f"CSVì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")
    st.stop()

# -------------------------
# í…ìŠ¤íŠ¸ ì •ê·œí™” / í† í°í™”
# -------------------------
def normalize_text_for_tokens(s: str):
    if not isinstance(s, str):
        return []
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\u3000", " ").replace("\xa0", " ").strip()
    s = re.sub(r'\(\+\s*\d+\s*\)', ' ', s)
    s = re.sub(r'\+\s*\d+', ' ', s)
    s = re.sub(r'\b[45]\s*ì„±\b', ' ', s)
    s = re.sub(r'[,\|/Â·;:()\[\]ã€ã€‘\-\â€”\_\/\\\*â€¢â€¦Â·"\'`~<>Â«Â»]', ' ', s)
    s = re.sub(r'[\U00010000-\U0010ffff]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', s)
    return [t.lower() for t in tokens if t.strip()]

def normalize_search_terms(raw_query: str):
    if not raw_query or not str(raw_query).strip():
        return []
    parts = [p for p in re.split(r'[\s,]+', raw_query.strip()) if p]
    normalized = []
    for p in parts:
        p2 = unicodedata.normalize("NFKC", p)
        p2 = p2.replace("\u3000", " ").replace("\xa0", " ").strip()
        p2 = re.sub(r'\b[45]\s*ì„±\b', '', p2)
        p2 = re.sub(r'\+\s*\d+', '', p2)
        p2 = re.sub(r'[^ê°€-í£A-Za-z0-9]', '', p2)
        p2 = p2.lower().strip()
        if p2:
            normalized.append(p2)
    return normalized

# -------------------------
# í† í° ì»¬ëŸ¼ ìƒì„±
# -------------------------
if "_tokens" not in df.columns:
    df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").astype(str).apply(normalize_text_for_tokens)

# -------------------------
# UI ìŠ¤íƒ€ì¼ (í…Œì´ë¸” ë„“ì´ ì¡°ì •)
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰", layout="wide")

st.markdown("""
    <style>
        .stTextInput { text-align: center; }
        div[data-testid="stTextInput"] label { display: none; }
        .block-container { display:flex; flex-direction:column; justify-content:center; align-items:center; }

        /* âœ… í‘œ ì „ì²´ í­ í™•ì¥ + ìºë¦­í„° ëª©ë¡ ë„“ê²Œ */
        div[data-testid="stDataFrame"] table {
            width: 100% !important;
        }
        div[data-testid="stDataFrame"] td {
            white-space: pre-wrap !important;
            word-break: break-word !important;
            line-height: 1.4em !important;
            font-size: 15px !important;
        }
        /* ìºë¦­í„° ëª©ë¡ ì—´ ë„“ì´: ìµœì†Œ 600px í™•ë³´ */
        div[data-testid="stDataFrame"] td:nth-child(4) {
            min-width: 600px !important;
            max-width: 900px !important;
        }
    </style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰</h2>", unsafe_allow_html=True)

if "search_clicked" not in st.session_state:
    st.session_state.search_clicked = False

# -------------------------
# ì…ë ¥ì°½ / í•„í„°
# -------------------------
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬ (ë„ì–´ì“°ê¸°ë¡œ ì—¬ëŸ¬ ìºë¦­í„° ê²€ìƒ‰)")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100), step=1)
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ê°œìˆ˜", min_value=0, max_value=100, value=0, step=1)

if st.button("ê²€ìƒ‰"):
    st.session_state.search_clicked = True

# -------------------------
# ê²€ìƒ‰ ë¡œì§
# -------------------------
if st.session_state.search_clicked:
    search_tokens = normalize_search_terms(query)
    filtered = df.copy()

    filtered["ê°€ê²©"] = pd.to_numeric(filtered["ê°€ê²©"], errors="coerce")
    filtered = filtered[(filtered["ê°€ê²©"] >= min_price) & (filtered["ê°€ê²©"] <= max_price)]

    filtered["í•œì •"] = pd.to_numeric(filtered["í•œì •"], errors="coerce").fillna(0)
    filtered = filtered[filtered["í•œì •"] >= min_limit]

    if search_tokens:
        def match_all_tokens(token_list):
            if not isinstance(token_list, (list, tuple)):
                return False
            return all(any(term == tok for tok in token_list) for term in search_tokens)
        filtered = filtered[filtered["_tokens"].apply(match_all_tokens)]

    if not filtered.empty:
        st.write(f"ğŸ” ì´ {len(filtered)}ê°œ ê³„ì •ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.dataframe(filtered[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]],
                     use_container_width=True, height=700)
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("**ë””ë²„ê·¸:** CSV ìƒìœ„ 10í–‰ í‘œì‹œ")
        st.dataframe(df.head(10), use_container_width=True, height=400)

# -------------------------
# ì‚¬ìš© ë°©ë²•
# -------------------------
st.markdown("""
---
### ğŸ’¡ ì‚¬ìš© ë°©ë²•
1ï¸âƒ£ ê²€ìƒ‰ì°½ì— ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•´ ì…ë ¥í•˜ì„¸ìš”.  
â€ƒâ€ƒì˜ˆ: `íˆë§ˆë¦¬ íˆì¹´ë¦¬ ë„¤ë£¨`  
2ï¸âƒ£ ê°€ê²©ëŒ€ ìŠ¬ë¼ì´ë”ë¡œ ì›í•˜ëŠ” ê°€ê²© ë²”ìœ„ë¥¼ ì„¤ì •í•˜ì„¸ìš”.  
3ï¸âƒ£ â€˜ìµœì†Œ í•œì • ìºë¦­í„° ê°œìˆ˜â€™ë¥¼ ì¡°ì •í•˜ì—¬ ì¡°ê±´ì„ ì„¸ë°€í•˜ê²Œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
4ï¸âƒ£ [ê²€ìƒ‰] ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ í‘œë¡œ í‘œì‹œë©ë‹ˆë‹¤.  
5ï¸âƒ£ ìºë¦­í„° ëª©ë¡ì€ í‘œì—ì„œ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
""", unsafe_allow_html=True)
