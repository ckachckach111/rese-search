import streamlit as st
import pandas as pd
import re
import unicodedata

# -------------------------
# ë°ì´í„° ë¡œë“œ (ì¸ì½”ë”© ì•ˆì „)
# -------------------------
@st.cache_data
def load_data(path="accounts.csv"):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df
        except Exception:
            continue
    return pd.DataFrame()

df = load_data()
if df.empty:
    st.error("accounts.csvë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ê³¼ ì¸ì½”ë”©ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -------------------------
# ì»¬ëŸ¼ ì´ë¦„ ë³´ì •
# -------------------------
df.columns = [c.strip().replace("ìºë¦­í„°ëª©ë¡", "ìºë¦­í„° ëª©ë¡") for c in df.columns]
required = ["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "ìºë¦­í„° ëª©ë¡"]
if any(c not in df.columns for c in required):
    st.error(f"CSV ì»¬ëŸ¼ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. í•„ìš”í•œ ì»¬ëŸ¼: {required} í˜„ì¬: {list(df.columns)}")
    st.stop()

# -------------------------
# ì •ê·œí™” & í† í°í™” í•¨ìˆ˜ (ê²€ìƒ‰ìš©)
# -------------------------
def normalize_text_for_tokens(s: str):
    if not isinstance(s, str):
        return []
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\u3000", " ").replace("\xa0", " ").strip()
    s = re.sub(r'\(\+\s*\d+\s*\)', ' ', s)
    s = re.sub(r'\+\s*\d+', ' ', s)
    s = re.sub(r'\b[45]\s*ì„±\b', ' ', s)
    s = re.sub(r'[,\|/Â·;:()\[\]ã€ã€‘\-\â€”\_\/\\\*â€¢â€¦Â·"\'`~<>Â«Â»]', ' ', s)
    s = re.sub(r'[\U00010000-\U0010ffff]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z0-9]+', s)
    return [t.lower() for t in tokens if t.strip()]

def normalize_search_terms(raw_query: str):
    if not raw_query or not str(raw_query).strip():
        return []
    parts = [p for p in re.split(r'[\s,]+', raw_query.strip()) if p]
    normalized = []
    for p in parts:
        p2 = unicodedata.normalize("NFKC", p)
        p2 = p2.replace("\u3000", " ").replace("\xa0", " ").strip()
        p2 = re.sub(r'\b[45]\s*ì„±\b', '', p2)
        p2 = re.sub(r'\+\s*\d+', '', p2)
        p2 = re.sub(r'[^ê°€-í£A-Za-z0-9]', '', p2)
        p2 = p2.lower().strip()
        if p2:
            normalized.append(p2)
    return normalized

# -------------------------
# í† í° ì»¬ëŸ¼ ìƒì„± (ê²€ìƒ‰ìš©)
# -------------------------
df["ìºë¦­í„° ëª©ë¡"] = df["ìºë¦­í„° ëª©ë¡"].fillna("").astype(str)
df["_tokens"] = df["ìºë¦­í„° ëª©ë¡"].apply(normalize_text_for_tokens)

# -------------------------
# íŒ¨ìŠ¤ ê°¯ìˆ˜: CSV ê°’ë§Œ ì‚¬ìš©
# -------------------------
pass_col = None
for c in ["íŒ¨ìŠ¤ ê°¯ìˆ˜", "íŒ¨ìŠ¤", "íŒ¨ìŠ¤ê°¯ìˆ˜"]:
    if c in df.columns:
        pass_col = c
        break

if pass_col is None:
    df["íŒ¨ìŠ¤"] = 0
else:
    df["íŒ¨ìŠ¤"] = pd.to_numeric(df[pass_col], errors="coerce").fillna(0).astype(int)

# -------------------------
# UI
# -------------------------
st.set_page_config(page_title="ê³„ì • ê²€ìƒ‰", layout="wide")
st.markdown("""
<style>
div[data-testid="stDataFrame"] table { width: 100% !important; }
div[data-testid="stDataFrame"] td { white-space: pre-wrap !important; word-break: break-word !important; line-height:1.35em !important; }
div[data-testid="stDataFrame"] td:nth-child(4) { min-width: 600px !important; max-width: 1000px !important; }
</style>
""", unsafe_allow_html=True)

st.markdown("<h2 style='text-align:center;'>ğŸ® ê³„ì • ê²€ìƒ‰</h2>", unsafe_allow_html=True)

# -------------------------
# ê²€ìƒ‰ ì¡°ê±´
# -------------------------
query = st.text_input("", "", placeholder="ì˜ˆ: íˆë§ˆë¦¬ íˆì¹´ë¦¬ (ë„ì–´ì“°ê¸°ë¡œ AND ê²€ìƒ‰)")
min_price, max_price = st.slider("ê°€ê²©ëŒ€ (ë§Œì›)", 0, 100, (0, 100))
min_limit = st.number_input("ìµœì†Œ í•œì • ìºë¦­í„° ìˆ˜", 0, 100, 0)

min_pass_possible = int(df["íŒ¨ìŠ¤"].min())
max_pass_possible = int(df["íŒ¨ìŠ¤"].max())
min_pass, max_pass = st.slider("íŒ¨ìŠ¤ ê°¯ìˆ˜", min_pass_possible, max_pass_possible, (min_pass_possible, max_pass_possible))

# -------------------------
# ê²€ìƒ‰ ì‹¤í–‰
# -------------------------
if st.button("ê²€ìƒ‰"):
    result = df.copy()

    result["ê°€ê²©"] = pd.to_numeric(result["ê°€ê²©"], errors="coerce")
    result = result[(result["ê°€ê²©"] >= min_price) & (result["ê°€ê²©"] <= max_price)]

    result["í•œì •"] = pd.to_numeric(result["í•œì •"], errors="coerce").fillna(0)
    result = result[result["í•œì •"] >= min_limit]

    result = result[(result["íŒ¨ìŠ¤"] >= min_pass) & (result["íŒ¨ìŠ¤"] <= max_pass)]

    terms = normalize_search_terms(query)
    if terms:
        result = result[result["_tokens"].apply(lambda t: all(term in t for term in terms))]

    if not result.empty:
        st.write(f"ğŸ” ì´ {len(result)}ê°œ ê³„ì •")
        st.dataframe(
            result[["ë²ˆí˜¸", "í•œì •", "ê°€ê²©", "íŒ¨ìŠ¤", "ìºë¦­í„° ëª©ë¡"]],
            use_container_width=True,
            height=700
        )
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤.")

# -------------------------
# ì‚¬ìš© ì„¤ëª… (í™”ë©´ í•˜ë‹¨)
# -------------------------
st.markdown("---")
st.markdown("""
### ì‚¬ìš© ë°©ë²•
- **ê²€ìƒ‰ì°½**: ìºë¦­í„° ì´ë¦„ì„ ë„ì–´ì“°ê¸°ë¡œ ì—¬ëŸ¬ ê°œ ì…ë ¥í•˜ë©´ *AND* ê²€ìƒ‰ë©ë‹ˆë‹¤.  
  ì˜ˆ: `íˆì¹´ë¦¬ íˆë§ˆë¦¬` â†’ ë‘ ì´ë¦„ ëª¨ë‘ í¬í•¨ëœ ê³„ì •ë§Œ í‘œì‹œë©ë‹ˆë‹¤.  
- **ê°€ê²©ëŒ€ (ë§Œì›)**: ìŠ¬ë¼ì´ë”ë¡œ ì›í•˜ëŠ” ê°€ê²© ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš” (ë‹¨ìœ„: ë§Œì›).  
- **ìµœì†Œ í•œì • ìºë¦­í„° ìˆ˜**: í•œì • ìºë¦­í„°(ìˆ«ì) ìµœì†Œ ê°œìˆ˜ ì§€ì •.  
- **íŒ¨ìŠ¤ ê°¯ìˆ˜**: CSVì— ì íŒ 'íŒ¨ìŠ¤ ê°¯ìˆ˜' ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¬ë¼ì´ë” í•„í„°ê°€ ë™ì‘í•©ë‹ˆë‹¤.  
  (CSV ì»¬ëŸ¼ëª…ì´ `íŒ¨ìŠ¤ ê°¯ìˆ˜`, `íŒ¨ìŠ¤`, `íŒ¨ìŠ¤ê°¯ìˆ˜` ì¤‘ í•˜ë‚˜ë©´ ìë™ ì¸ì‹í•©ë‹ˆë‹¤.)  
- **ê²€ìƒ‰ ë²„íŠ¼**: ì¡°ê±´ì„ ì„¤ì •í•œ ë’¤ 'ê²€ìƒ‰'ì„ í´ë¦­í•˜ë©´ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.  
- **ê²°ê³¼ í‘œ ì»¬ëŸ¼**: `ë²ˆí˜¸`, `í•œì •`, `ê°€ê²©`, `íŒ¨ìŠ¤`, `ìºë¦­í„° ëª©ë¡` ìˆœìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.

> ë¬¸ì œ ë°œìƒ ì‹œ: CSVì˜ í—¤ë”(ì²« ì¤„)ì™€ ìƒìœ„ 5ê°œ í–‰ì„ ë³µì‚¬í•´ì„œ ë³´ë‚´ì£¼ì‹œë©´ ë°”ë¡œ í™•ì¸í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
""")
